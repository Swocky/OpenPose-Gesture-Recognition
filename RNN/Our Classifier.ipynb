{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import random, time, os, json, glob, re, time\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 37\n",
    "n_classes = 27\n",
    "db_type = 'all' #body | hands | all\n",
    "\n",
    "train_labels = '../labels/train.csv'\n",
    "val_labels = '../labels/validation.csv'\n",
    "all_labels = '../labels/labels.csv'\n",
    "\n",
    "labels = pd.read_csv(train_labels, index_col=0, sep=';', header=None)\n",
    "labels = json.loads(labels.to_json())['1']\n",
    "\n",
    "validation_labels = pd.read_csv(val_labels, index_col=0, sep=';', header=None)\n",
    "validation_labels = json.loads(validation_labels.to_json())['1']\n",
    "\n",
    "labels.update(validation_labels)\n",
    "id_label_dict = labels\n",
    "df = pd.read_csv(all_labels, header = None).reset_index()\n",
    "label_class_dict = dict(df[[0,'index']].values)\n",
    "\n",
    "def get_class(file):\n",
    "  f_id = re.findall(r'([0-9]*).npy', file)[0]\n",
    "  f_label = id_label_dict[f_id]\n",
    "  f_class = label_class_dict[f_label]\n",
    "  \n",
    "  return f_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = join('../../data/train/',db_type,'*.npy')\n",
    "files = glob.glob(file_path)\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.array([get_class(file) for file in files])\n",
    "\n",
    "\n",
    "# Make a one hot encoding of the labels\n",
    "y_tmp = np.zeros((len(y_data), n_classes), dtype = np.int32)\n",
    "for i in range(len(y_data)):\n",
    "  y_tmp[i,y_data[i]] = 1\n",
    "\n",
    "y_data = y_tmp\n",
    "  \n",
    "X_data = np.r_[[np.load(files[i]) for i in range(len(files))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = 13000\n",
    "\n",
    "\n",
    "permute = np.random.permutation(y_data.shape[0])\n",
    "y_data = y_data[permute]\n",
    "X_data = X_data[permute]\n",
    "\n",
    "y_train, y_test = y_data[:train_size], y_data[train_size:]\n",
    "X_train, X_test = X_data[:train_size], X_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data \n",
    "tf.reset_default_graph()\n",
    "\n",
    "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 1197 test series\n",
    "n_input = len(X_train[0][0])  # num input parameters per timestep\n",
    "\n",
    "n_hidden = [90,50] # Hidden layer num of features\n",
    "\n",
    "#updated for learning-rate decay\n",
    "# calculated as: decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "decaying_learning_rate = True\n",
    "learning_rate = 0.0025 #used if decaying_learning_rate set to False\n",
    "init_learning_rate = 0.005\n",
    "decay_rate = 0.96 #the base of the exponential in the decay\n",
    "decay_steps = 100000 #used in decay every 60000 steps with a base of 0.96\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "lambda_loss_amount = 0.0015\n",
    "\n",
    "epochs = 20\n",
    "training_iters = training_data_count *300  # Loop 300 times on the dataset, ie 300 epochs\n",
    "batch_size = 512\n",
    "display_iter = batch_size*64 # To show test set accuracy during training\n",
    "\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print((X_train.shape, y_test.shape, np.mean(X_test), np.std(X_test)))\n",
    "print(\"\\nThe dataset has not been preprocessed, is not normalised etc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_batch_size(_train, _labels, _unsampled, batch_size):\n",
    "    # Fetch a \"batch_size\" amount of data and labels from \"(X|y)_train\" data. \n",
    "    # Elements of each batch are chosen randomly, without replacement, from X_train with corresponding label from Y_train\n",
    "    # unsampled_indices keeps track of sampled data ensuring non-replacement. Resets when remaining datapoints < batch_size    \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "    batch_labels = np.empty((batch_size, n_classes)) \n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        # index = random sample from _unsampled (indices)\n",
    "        index = random.choice(_unsampled)\n",
    "        batch_s[i] = _train[index] \n",
    "        batch_labels[i] = _labels[index]\n",
    "        _unsampled.remove(index)\n",
    "\n",
    "\n",
    "    return batch_s, batch_labels, _unsampled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# The graph\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.int32, [None, n_classes])\n",
    "#y_hot = tf.one_hot(tf.cast(y,tf.int32),n_classes)\n",
    "\n",
    "\n",
    "with tf.name_scope('Input_Layer') as _:\n",
    "  xt = tf.transpose(x, [1, 0, 2])  # permute n_steps and batch_size\n",
    "  xr = tf.reshape(xt, [-1, n_input])  \n",
    "  # Rectifies Linear Unit activation function used\n",
    "    \n",
    "with tf.name_scope('First_Layer') as _:\n",
    "  y_1 = tf.layers.dense(xr, n_hidden[0], \n",
    "  bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                  activation=tf.nn.relu, name='layer_1')\n",
    "    \n",
    "  # So the reader might be wondering why we take the transpose before the reshape\n",
    "  # This has to do with how the lstm wants to eat the data.\n",
    "  # the lstm want n_frames tensors, where each tensor is of size batchsize x num_features\n",
    "  # So basically first all the 1st frames then all the 2nd frames etc.\n",
    "  _X_split = tf.split(y_1, n_steps, 0) \n",
    "\n",
    "    \n",
    "with tf.name_scope('LSTM') as _:\n",
    "  # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "  lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden[0], forget_bias=1.0, state_is_tuple=True)\n",
    "  lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden[0], forget_bias=1.0, state_is_tuple=True)\n",
    "  lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "  outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X_split, dtype=tf.float32)\n",
    "    \n",
    "  # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
    "  lstm_last_output = outputs[-1]\n",
    "    \n",
    "    \n",
    "with tf.name_scope('Second_Layer') as _:\n",
    "  y_2 = tf.layers.dense(lstm_last_output, n_hidden[1], \n",
    "                          bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          activation=tf.nn.relu, name='layer_2')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('Output_layer') as _:\n",
    "  pred = tf.layers.dense(y_2, n_classes, \n",
    "                                  bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                  activation=None, \n",
    "                                  name='final_layer')\n",
    "\n",
    "\n",
    "with tf.name_scope('Optimizer') as _:\n",
    "  # Loss, optimizer and evaluation\n",
    "  # The regularization term\n",
    "  l2 = lambda_loss_amount * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()) \n",
    "  \n",
    "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                          labels=y, \n",
    "                          logits=pred)) + l2 # Softmax loss\n",
    "  if decaying_learning_rate:\n",
    "    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "\n",
    "  #decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) #exponentially decayed learning rate\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step) # Adam Optimizer\n",
    "  tf.summary.scalar('cost', cost)\n",
    "  \n",
    "  \n",
    "with tf.name_scope('Accuracy') as _:\n",
    "  correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "  tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inititialize writers\n",
    "time_now = datetime.datetime.strftime(datetime.datetime.now(), '%H%M%S')\n",
    "summary_dir = './logs/'+db_type+time_now\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(summary_dir + '/train')\n",
    "test_writer=  tf.summary.FileWriter(summary_dir + '/test')\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of data at each loop. \n",
    "# Elements of each batch are chosen randomly, without replacement, from X_train, \n",
    "# restarting when remaining datapoints < batch_size\n",
    "step = 1\n",
    "best_test_acc = 0\n",
    "time_start = time.time()\n",
    "unsampled_indices = list(range(0,len(X_train)))\n",
    "start = time.time()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  sess.graph.finalize()\n",
    "  \n",
    "  while step * batch_size <= training_iters:\n",
    "    if len(unsampled_indices) < batch_size:\n",
    "      unsampled_indices = list(range(0,len(X_train))) \n",
    "    batch_xs, batch_ys, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
    "    \n",
    "    # Fit training using batch data\n",
    "    _, loss_trn, acc_trn = sess.run([optimizer, cost, accuracy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "    \n",
    "    summary = sess.run(merged, feed_dict={x: batch_xs, y: batch_ys}) \n",
    "    train_writer.add_summary(summary, step)\n",
    "    \n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss_tst, acc_tst = sess.run([cost, accuracy], feed_dict={x: X_test, y: y_test})\n",
    "        summary = sess.run(merged, feed_dict={x: X_test, y: y_test})\n",
    "        \n",
    "        test_writer.add_summary(summary, step)\n",
    "        \n",
    "        \n",
    "        print('\\r Iter: {},  Accuracy test: {}, Accuracy train: {}, time: {}'.format(step*batch_size, acc_tst, acc_trn, time.time()-start))\n",
    "        \n",
    "        if(acc_tst > best_test_acc):\n",
    "          print(\"\\tbetter network stored,\", acc_tst, \">\", best_test_acc)\n",
    "          best_test_acc = acc_tst\n",
    "          saver.save(sess=sess, save_path=summary_dir + '/bestNetwork')\n",
    "          \n",
    "        \n",
    "        start = time.time()\n",
    "    step += 1\n",
    "  print(\"Optimization Finished!\")\n",
    "\n",
    "  # Accuracy for test data\n",
    "  \n",
    "  saver.restore(sess=sess, save_path=summary_dir + '/bestNetwork')\n",
    "  print(\"best training accuracy:\", sess.run(accuracy, feed_dict={x: X_train, y: y_train}),\n",
    "  \"best test accuracy: \", sess.run(accuracy, feed_dict={x: X_test, y: X_test}))\n",
    "\n",
    "time_stop = time.time()\n",
    "print((\"TOTAL TIME:  {}\".format(time_stop - time_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def LSTM_RNN(_X):\n",
    "    # model architecture based on \"guillaume-chevalier\" and \"aymericdamien\" under the MIT license.\n",
    "    \n",
    "    with tf.name_scope('Input_Layer') as _:\n",
    "      _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "      _X = tf.reshape(_X, [-1, n_input])  \n",
    "      # Rectifies Linear Unit activation function used\n",
    "    \n",
    "    with tf.name_scope('First_Layer') as _:\n",
    "      y_1 = tf.layers.dense(_X, n_hidden[0], \n",
    "                          bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          activation=tf.nn.relu, name='layer_1')\n",
    "    \n",
    "      # So the reader might be wondering why we take the transpose before the reshape\n",
    "      # This has to do with how the lstm wants to eat the data.\n",
    "      # the lstm want n_frames tensors, where each tensor is of size batchsize x num_features\n",
    "      # So basically first all the 1st frames then all the 2nd frames etc.\n",
    "      _X_split = tf.split(y_1, n_steps, 0) \n",
    "\n",
    "    \n",
    "    with tf.name_scope('LSTM') as _:\n",
    "      # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "      lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden[0], forget_bias=1.0, state_is_tuple=True)\n",
    "      lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden[0], forget_bias=1.0, state_is_tuple=True)\n",
    "      lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "      outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X_split, dtype=tf.float32)\n",
    "    \n",
    "      # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
    "      lstm_last_output = outputs[-1]\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('Second_Layer') as _:\n",
    "      y_2 = tf.layers.dense(lstm_last_output, n_hidden[1], \n",
    "                          bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          activation=tf.nn.relu, name='layer_2')\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('Output_layer') as _:\n",
    "      output = tf.layers.dense(y_2, n_classes, \n",
    "                                  bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                  activation=None, \n",
    "                                  name='final_layer')\n",
    "    \n",
    "    \n",
    "    return output\n",
    "    \n",
    "    #batch_ys = one_hot(raw_labels)\n",
    "    \n",
    "    # check that encoded output is same length as num_classes, if not, pad it \n",
    "    #if len(batch_ys[0]) < n_classes:\n",
    "    #    temp_ys = np.zeros((batch_size, n_classes))\n",
    "    #    temp_ys[:batch_ys.shape[0],:batch_ys.shape[1]] = batch_ys\n",
    "    #    batch_ys = temp_ys"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def _get_video(file):\n",
    "  file = file.decode('utf-8')\n",
    "  x = np.load(file)\n",
    "  y = get_class(file)\n",
    "  return x,y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with tf.name_scope('training_data') as scope:\n",
    "  ds = tf.data.Dataset.from_tensor_slices((files))\n",
    "  ds = ds.map(lambda file:tf.py_func(_get_video,[file],[tf.float64,tf.int64]))\n",
    "  ds = ds.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=2*batch_size, count = epochs))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(1)\n",
    "  ds_iterator = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)\n",
    "  \n",
    "  ds_next_element = ds_iterator.get_next()\n",
    "  ds_init_op = ds_iterator.make_initializer(ds)\n",
    "  \n",
    "  def one_hot(y_):\n",
    "    # One hot encoding of the network outputs\n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
