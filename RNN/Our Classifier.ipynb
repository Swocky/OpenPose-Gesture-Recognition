{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import random, time, os, json, glob, re, time\n",
    "from random import randint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 37\n",
    "\n",
    "train_labels = '../labels/train.csv'\n",
    "val_labels = '../labels/validation.csv'\n",
    "all_labels = '../labels/labels.csv'\n",
    "\n",
    "labels = pd.read_csv(train_labels, index_col=0, sep=';', header=None)\n",
    "labels = json.loads(labels.to_json())['1']\n",
    "\n",
    "validation_labels = pd.read_csv(val_labels, index_col=0, sep=';', header=None)\n",
    "validation_labels = json.loads(validation_labels.to_json())['1']\n",
    "\n",
    "labels.update(validation_labels)\n",
    "id_label_dict = labels\n",
    "df = pd.read_csv(all_labels, header = None).reset_index()\n",
    "label_class_dict = dict(df[[0,'index']].values)\n",
    "\n",
    "def get_class(file):\n",
    "  f_id = re.findall(r'([0-9]*).npy', file)[0]\n",
    "  f_label = id_label_dict[f_id]\n",
    "  f_class = label_class_dict[f_label]\n",
    "  \n",
    "  return f_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12023"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('../../data/train/*.npy')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = np.array([[get_class(file)] for file in files])\n",
    "\n",
    "X_data = np.r_[[np.load(files[i]) for i in range(len(files))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permute = np.random.permutation(y_data.shape[0])\n",
    "y_data = y_data[permute]\n",
    "X_data = X_data[permute]\n",
    "\n",
    "y_train, y_test = y_data[:10000], y_data[10000:]\n",
    "X_train, X_test = X_data[:10000], X_data[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1), (10000, 37, 120), (2023, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "((10000, 37, 120), (2023, 1), 26.679430246985433, 40.321621301793)\n",
      "\n",
      "The dataset has not been preprocessed, is not normalised etc\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "tf.reset_default_graph()\n",
    "\n",
    "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 1197 test series\n",
    "n_input = len(X_train[0][0])  # num input parameters per timestep\n",
    "\n",
    "n_hidden = [90,50] # Hidden layer num of features\n",
    "n_classes = 27 \n",
    "\n",
    "#updated for learning-rate decay\n",
    "# calculated as: decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "decaying_learning_rate = True\n",
    "learning_rate = 0.0025 #used if decaying_learning_rate set to False\n",
    "init_learning_rate = 0.005\n",
    "decay_rate = 0.96 #the base of the exponential in the decay\n",
    "decay_steps = 100000 #used in decay every 60000 steps with a base of 0.96\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "lambda_loss_amount = 0.0015\n",
    "\n",
    "epochs = 300\n",
    "training_iters = training_data_count *300  # Loop 300 times on the dataset, ie 300 epochs\n",
    "batch_size = 512\n",
    "display_iter = batch_size*64 # To show test set accuracy during training\n",
    "\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print((X_train.shape, y_test.shape, np.mean(X_test), np.std(X_test)))\n",
    "print(\"\\nThe dataset has not been preprocessed, is not normalised etc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X):\n",
    "    # model architecture based on \"guillaume-chevalier\" and \"aymericdamien\" under the MIT license.\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    _X = tf.reshape(_X, [-1, n_input])  \n",
    "    # Rectifies Linear Unit activation function used\n",
    "    y_1 = tf.layers.dense(_X, n_hidden[0], \n",
    "                          bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          activation=tf.nn.relu, name='layer_1')\n",
    "    \n",
    "    # So the reader might be wondering why we take the transpose before the reshape\n",
    "    # This has to do with how the lstm wants to eat the data.\n",
    "    # the lstm want n_frames tensors, where each tensor is of size batchsize x num_features\n",
    "    # So basically first all the 1st frames then all the 2nd frames etc.\n",
    "    _X_split = tf.split(y_1, n_steps, 0) \n",
    "\n",
    "    \n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden[0], forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden[0], forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X_split, dtype=tf.float32)\n",
    "    \n",
    "    # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    y_2 = tf.layers.dense(lstm_last_output, n_hidden[1], \n",
    "                          bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          activation=tf.nn.relu, name='layer_2')\n",
    "    \n",
    "    \n",
    "    output = tf.layers.dense(y_2, n_classes, \n",
    "                                  bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                  activation=None, \n",
    "                                  name='final_layer')\n",
    "    return output\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, _labels, _unsampled, batch_size):\n",
    "    # Fetch a \"batch_size\" amount of data and labels from \"(X|y)_train\" data. \n",
    "    # Elements of each batch are chosen randomly, without replacement, from X_train with corresponding label from Y_train\n",
    "    # unsampled_indices keeps track of sampled data ensuring non-replacement. Resets when remaining datapoints < batch_size    \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "    batch_labels = np.empty((batch_size,1)) \n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        # index = random sample from _unsampled (indices)\n",
    "        index = random.choice(_unsampled)\n",
    "        batch_s[i] = _train[index] \n",
    "        batch_labels[i] = _labels[index]\n",
    "        _unsampled.remove(index)\n",
    "\n",
    "\n",
    "    return batch_s, batch_labels, _unsampled\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # One hot encoding of the network outputs\n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def _get_video(file):\n",
    "  file = file.decode('utf-8')\n",
    "  x = np.load(file)\n",
    "  y = get_class(file)\n",
    "  return x,y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with tf.name_scope('training_data') as scope:\n",
    "  ds = tf.data.Dataset.from_tensor_slices((files))\n",
    "  ds = ds.map(lambda file:tf.py_func(_get_video,[file],[tf.float64,tf.int64]))\n",
    "  ds = ds.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=2*batch_size, count = epochs))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(1)\n",
    "  ds_iterator = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)\n",
    "  \n",
    "  ds_next_element = ds_iterator.get_next()\n",
    "  ds_init_op = ds_iterator.make_initializer(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "pred = LSTM_RNN(x)\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "if decaying_learning_rate:\n",
    "    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "\n",
    "#decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) #exponentially decayed learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step) # Adam Optimizer\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "unsampled_indices = list(range(0,len(X_train)))\n",
    "sample_X, _, _ = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #512:  Learning rate = 0.005000:   Batch Loss = 3.765934, Accuracy = 0.025390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 3.7118875980377197, Accuracy = 0.05388037487864494\n",
      "Iter #32768:  Learning rate = 0.005000:   Batch Loss = 3.015604, Accuracy = 0.16796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 3.0834388732910156, Accuracy = 0.14434008300304413\n",
      "Iter #65536:  Learning rate = 0.005000:   Batch Loss = 2.823562, Accuracy = 0.212890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.8091678619384766, Accuracy = 0.23035097122192383\n",
      "Iter #98304:  Learning rate = 0.005000:   Batch Loss = 2.550646, Accuracy = 0.298828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.611361265182495, Accuracy = 0.2753336727619171\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "# sess.run(ds_init_op)\n",
    "# Perform Training steps with \"batch_size\" amount of data at each loop. \n",
    "# Elements of each batch are chosen randomly, without replacement, from X_train, \n",
    "# restarting when remaining datapoints < batch_size\n",
    "step = 1\n",
    "time_start = time.time()\n",
    "unsampled_indices = list(range(0,len(X_train)))\n",
    "\n",
    "while step * batch_size <= training_iters:\n",
    "    #print (sess.run(learning_rate)) #decaying learning rate\n",
    "    #print (sess.run(global_step)) # global number of iterations\n",
    "    if len(unsampled_indices) < batch_size:\n",
    "      unsampled_indices = list(range(0,len(X_train))) \n",
    "    batch_xs, raw_labels, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
    "    \n",
    "    #batch_xs, raw_labels = sess.run(ds_next_element)\n",
    "    \n",
    "    batch_ys = one_hot(raw_labels)\n",
    "    \n",
    "    \n",
    "    # check that encoded output is same length as num_classes, if not, pad it \n",
    "    if len(batch_ys[0]) < n_classes:\n",
    "        temp_ys = np.zeros((batch_size, n_classes))\n",
    "        temp_ys[:batch_ys.shape[0],:batch_ys.shape[1]] = batch_ys\n",
    "        batch_ys = temp_ys\n",
    "       \n",
    "    \n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print((\"Iter #\" + str(step*batch_size) + \\\n",
    "              \":  Learning rate = \" + \"{:.6f}\".format(sess.run(learning_rate)) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc)))\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print((\"PERFORMANCE ON TEST SET:             \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc)))\n",
    "\n",
    "    step += 1\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: one_hot(y_test)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print((\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy)))\n",
    "time_stop = time.time()\n",
    "print((\"TOTAL TIME:  {}\".format(time_stop - time_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unsampled_indices = list(range(0,len(X_train)))\n",
    "sample_X, _, _ = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.get_default_graph\n",
    "\n",
    "x = tf.constant(sample_X)\n",
    "xt = tf.transpose(x, [1,0,2])\n",
    "x1 = tf.reshape(xt ,[-1,120])\n",
    "\n",
    "# here is the relu dense layer\n",
    "\n",
    "xs = tf.split(x1,37)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "x_split, x_tran = sess.run([xs,xt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(x_split).shape, np.array(x_org).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array_equal(x_split,x_tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
